# Haskell ASTChunking Enhancement

## Overview

This document describes the comprehensive enhancement of Haskell code chunking in CocoIndex, inspired by techniques from the ASTChunk library. The improvements transform the original basic regex-based approach into a sophisticated, configurable chunking system with rich metadata and intelligent boundary detection.

## Problem Statement

The original Haskell chunking implementation (`haskell_support.py`) had several limitations:

- **Fixed configuration**: Hardcoded chunk parameters with no customization options
- **Basic metadata**: Limited location information without content analysis
- **Simple fallback**: Single regex-based fallback with basic separators
- **No size control**: No adaptive chunk sizing or optimization
- **Limited context**: No overlap or expansion capabilities
- **Basic separators**: Simple list without priority or scoring

## Solution: ASTChunk-Inspired Enhancement

### Architecture Overview

The enhanced system introduces a multi-layered architecture with configurable chunking strategies:

```
┌─────────────────────────────────────────────────────────┐
│                 EnhancedHaskellChunker                  │
├─────────────────────────────────────────────────────────┤
│  Configuration Layer (HaskellChunkConfig)              │
│  ├─ Chunk size limits                                  │
│  ├─ Overlap settings                                   │
│  ├─ Metadata templates                                 │
│  └─ Haskell-specific options                          │
├─────────────────────────────────────────────────────────┤
│  Chunking Strategy Pipeline                            │
│  ├─ 1. AST-based chunking (tree-sitter)              │
│  ├─ 2. Size optimization & intelligent splitting      │
│  ├─ 3. Overlap addition (optional)                    │
│  ├─ 4. Context expansion (optional)                   │
│  └─ 5. Rich metadata enhancement                      │
├─────────────────────────────────────────────────────────┤
│  Fallback Strategy                                     │
│  ├─ Enhanced regex chunking (priority separators)     │
│  └─ Simple text chunking (last resort)                │
└─────────────────────────────────────────────────────────┘
```

## Key Improvements

### 1. Configuration System

**`HaskellChunkConfig` Class**
```python
class HaskellChunkConfig:
    def __init__(self, 
                 max_chunk_size: int = 1800,
                 chunk_overlap: int = 0,
                 chunk_expansion: bool = False,
                 metadata_template: str = "default",
                 preserve_imports: bool = True,
                 preserve_exports: bool = True):
```

**Benefits:**
- Centralized configuration management
- Template-based metadata generation
- Haskell-specific preservation options
- Full compatibility with ASTChunk patterns

### 2. Intelligent Size Optimization

**Adaptive Chunk Splitting**
- Large chunks automatically split at optimal boundaries
- Smart split point detection using separator priority scoring
- Size limits enforced while respecting code structure

**Split Point Algorithm:**
```python
def _find_best_split_point(self, lines, target_idx, separators):
    # Score-based approach:
    # - Higher scores for important separators (modules, imports)
    # - Distance penalty from target split point
    # - Preference for structural boundaries
```

### 3. Enhanced Separator System

**Priority-Ordered Separators:**
```python
enhanced_separators = [
    # High priority: Module and import boundaries
    r"\nmodule\s+[A-Z][a-zA-Z0-9_.']*",
    r"\nimport\s+(qualified\s+)?[A-Z][a-zA-Z0-9_.']*",
    
    # Medium priority: Type and data definitions
    r"\ndata\s+[A-Z][a-zA-Z0-9_']*",
    r"\nclass\s+[A-Z][a-zA-Z0-9_']*",
    
    # Lower priority: Function definitions
    r"\n[a-zA-Z][a-zA-Z0-9_']*\s*::",
    
    # Comment-based separators
    r"\n--\s*[=-]{3,}",
]
```

**Improvements:**
- Haskell-specific language constructs recognized
- Hierarchical priority system prevents bad splits
- Comment blocks used as natural boundaries

### 4. Rich Metadata Templates

**Default Template:**
```python
metadata = {
    "chunk_id": chunk.get("chunk_id", 0),
    "chunk_method": "haskell_ast",
    "language": "Haskell",
    "chunk_size": len(chunk["content"]),
    "non_whitespace_size": calculated_size,
    "line_count": len(chunk["content"].split('\n')),
    "start_line": chunk.get("start_line", 0),
    "end_line": chunk.get("end_line", 0),
    "node_type": chunk.get("node_type", "unknown"),
    "has_imports": "import " in chunk["content"],
    "has_type_signatures": "::" in chunk["content"],
    # ... additional Haskell-specific analysis
}
```

**RepoEval Template:**
- Extracted function names from type signatures
- Extracted type definitions (data, newtype, class)
- Dependency analysis from imports

**SWebench Template:**
- Complexity scoring based on Haskell constructs
- Monadic operation counting
- Control flow analysis

### 5. Context Enhancement Features

**Chunk Overlap:**
```python
def _add_chunk_overlap(self, chunks, content):
    # Add configurable line overlap between chunks
    # Maintains context across chunk boundaries
    # Preserves function/type relationships
```

**Chunk Expansion:**
```python
def _expand_chunks_with_context(self, chunks, file_path):
    # Add contextual headers to chunks
    # Format: "-- File: path | Lines: X-Y | Node type: Z"
    # Similar to ASTChunk expansion headers
```

### 6. Comprehensive Fallback Strategy

**Three-Tier Fallback System:**

1. **AST-based chunking** (tree-sitter): Primary method with full syntax awareness
2. **Enhanced regex chunking**: Priority-based separators with size optimization
3. **Simple text chunking**: Basic line-based splitting as last resort

**Enhanced Regex Fallback:**
```python
def create_enhanced_regex_fallback_chunks(content, file_path, config):
    # Uses priority-scored separators
    # Enforces size limits with intelligent splitting
    # Provides rich metadata even in fallback mode
    # Maintains Haskell-specific content analysis
```

### 7. Advanced Haskell Analysis

**Content Analysis Functions:**
```python
def _extract_function_names(self, content):
    # Regex: r'^([a-zA-Z][a-zA-Z0-9_\']*)\s*::'
    
def _extract_type_names(self, content):
    # Patterns for data, newtype, type, class definitions
    
def _calculate_complexity(self, content):
    # Counts: case, if, where, let, do, monadic ops
    
def _extract_dependencies(self, content):
    # Parses import statements for module dependencies
```

## Usage Examples

### Basic Usage
```python
# Simple chunking with defaults
chunker = EnhancedHaskellChunker()
chunks = chunker.chunk_code(haskell_code, "Main.hs")
```

### Advanced Configuration
```python
# Custom configuration for specific use case
config = HaskellChunkConfig(
    max_chunk_size=500,
    chunk_overlap=3,
    chunk_expansion=True,
    metadata_template="repoeval"
)
chunker = EnhancedHaskellChunker(config)
chunks = chunker.chunk_code(haskell_code, "Main.hs")
```

### CocoIndex Operation
```python
# Use as CocoIndex operation
@cocoindex.operation
def process_haskell_files():
    return extract_haskell_ast_chunks(
        content=haskell_source,
        config={
            "max_chunk_size": 800,
            "metadata_template": "swebench",
            "chunk_expansion": True
        }
    )
```

## Performance Improvements

### Caching System
- Builder instance caching for repeated operations
- Expensive regex compilation cached
- Separator matching optimized

### Memory Efficiency
- Streaming-based processing for large files
- Lazy evaluation of metadata
- Minimal memory footprint during chunking

## Comparison: Before vs After

| Feature | Original Implementation | Enhanced Implementation |
|---------|------------------------|------------------------|
| **Configuration** | Hardcoded parameters | Fully configurable via `HaskellChunkConfig` |
| **Metadata** | Basic location info only | Rich templates (default/repoeval/swebench) |
| **Size Control** | No size management | Adaptive splitting with intelligent boundaries |
| **Separators** | Simple regex list | Priority-ordered with scoring system |
| **Fallbacks** | Single regex fallback | Three-tier strategy (AST→Enhanced Regex→Text) |
| **Context** | No context preservation | Configurable overlap and expansion |
| **Analysis** | Basic AST node info | Deep Haskell construct analysis |
| **Performance** | No optimization | Caching and streaming optimizations |
| **Extensibility** | Fixed implementation | Template-based and configurable |

## Haskell-Specific Enhancements

### Language Construct Recognition
- **Module boundaries**: Preserved as high-priority separators
- **Import blocks**: Kept together when `preserve_imports=True`
- **Type signatures**: Recognized and analyzed for function extraction
- **Data types**: Detected and extracted for type analysis
- **Type classes**: Identified as important structural boundaries
- **Instance declarations**: Recognized for complexity analysis

### Complexity Scoring
The enhanced system provides Haskell-specific complexity metrics:
- Monadic operations (`>>`, `>>=`)
- Control structures (`case`, `if`, `where`, `let`, `do`)
- Function application operators (`$`)
- Type signature density (`::`counts)

### Metadata Analysis
Rich content analysis provides insights into chunk characteristics:
- `has_imports`: Contains import statements
- `has_exports`: Contains module export lists
- `has_type_signatures`: Contains function type declarations
- `has_data_types`: Contains data/newtype/type definitions
- `has_instances`: Contains type class instances
- `has_classes`: Contains type class definitions

## Integration with CocoIndex

### Backward Compatibility
- Original `extract_haskell_ast_chunks` function maintained
- Legacy return format supported via conversion layer
- Existing CocoIndex operations continue to work

### New Operations
- `EnhancedHaskellChunk`: Full-featured operation with all new capabilities
- `get_haskell_language_spec`: Enhanced language specification
- Template-based metadata for different use cases

### Configuration Integration
```python
# Enhanced language spec with configuration
spec = get_haskell_language_spec(
    config=HaskellChunkConfig(
        max_chunk_size=1000,
        chunk_expansion=True,
        metadata_template="repoeval"
    )
)
```

## Testing and Validation

### Test Coverage
The enhanced system includes comprehensive test coverage:
- Unit tests for each chunking strategy
- Integration tests with various Haskell code patterns
- Performance benchmarks against original implementation
- Metadata validation tests

### Example Test Cases
```python
def test_enhanced_haskell_chunking():
    # Tests multiple configurations
    # Validates metadata richness
    # Checks boundary detection accuracy
    # Verifies fallback behavior
```

## Future Enhancements

### Potential Improvements

#### 1. Direct Tree-sitter Integration

**Current**: Indirect usage via `haskell_tree_sitter` wrapper
**Future**: Direct tree-sitter Python bindings integration

```python
# Example: Direct AST node chunking
def _direct_ast_chunking(self, content: str):
    parser = Parser()
    parser.set_language(Language('haskell.so'))
    tree = parser.parse(bytes(content, "utf8"))
    
    chunks = []
    for node in tree.root_node.children:
        if node.type in ['function_declaration', 'data_declaration']:
            chunk = {
                "content": content[node.start_byte:node.end_byte],
                "ast_node_type": node.type,
                "ast_children": [child.type for child in node.children],
                "syntax_errors": node.has_error
            }
            chunks.append(chunk)
```

**Benefits**:

- More granular AST control
- Custom node traversal strategies  
- Better error handling for malformed code
- Richer AST metadata extraction

#### 2. Semantic Chunking
**Current**: Syntax-based boundaries (imports, functions, types)
**Future**: Meaning-based grouping using code analysis

```python
# Example: Function dependency-based chunking
def _semantic_dependency_chunking(self, content: str):
    functions = self._parse_functions(content)
    call_graph = self._build_call_graph(functions)
    
    # Group functions by dependency clusters
    clusters = self._find_dependency_clusters(call_graph)
    
    semantic_chunks = []
    for cluster in clusters:
        # Combine related functions into logical chunks
        chunk_content = self._combine_functions(cluster.functions)
        metadata = {
            "semantic_type": "dependency_cluster",
            "cluster_functions": [f.name for f in cluster.functions],
            "external_dependencies": cluster.external_calls,
            "cluster_complexity": cluster.cyclomatic_complexity
        }
        semantic_chunks.append({"content": chunk_content, "metadata": metadata})
```

**Advanced Semantic Strategies**:

- **Type-based grouping**: Group data types with their related functions
- **Module purpose analysis**: Identify utility vs business logic vs IO functions
- **Dependency minimization**: Create chunks with minimal cross-references
- **Conceptual clustering**: Group by domain concepts (user management, payment processing)

#### 3. Documentation Preservation

Special handling for Haddock documentation comments

```python
# Preserve documentation with related code
def _preserve_haddock_docs(self, chunks):
    for chunk in chunks:
        # Find preceding Haddock comments
        docs = self._extract_haddock_for_chunk(chunk)
        if docs:
            chunk["content"] = docs + "\n" + chunk["content"]
            chunk["metadata"]["has_documentation"] = True
            chunk["metadata"]["doc_coverage"] = len(docs.split('\n'))
```

#### 4. Module Graph Analysis

Cross-module dependency consideration for better chunking decisions

```python
# Consider imports when chunking
def _module_aware_chunking(self, content: str, module_context: dict):
    imports = self._extract_imports(content)
    
    for chunk in chunks:
        # Analyze which imports are actually used in this chunk
        used_imports = self._find_used_imports(chunk, imports)
        chunk["metadata"]["required_imports"] = used_imports
        chunk["metadata"]["import_density"] = len(used_imports) / len(imports)
```

#### 5. Performance Profiling Integration

Hot path optimization for large codebases

```python
# Profile-guided chunking optimization
def _performance_aware_chunking(self, content: str, profile_data: dict):
    # Use profiling data to inform chunking decisions
    hot_functions = profile_data.get("hot_functions", [])
    
    for chunk in chunks:
        chunk_functions = self._extract_function_names(chunk["content"])
        hotness_score = sum(1 for f in chunk_functions if f in hot_functions)
        chunk["metadata"]["performance_hotness"] = hotness_score
```

### Extension Points

- Custom metadata templates via plugin system
- Additional separator patterns for domain-specific code
- Integration with Haskell Language Server for semantic information
- Support for literate Haskell (.lhs) files

## Key Concepts Explained

### Direct Tree-sitter Integration vs Current Approach

**Current State**: We use tree-sitter indirectly through the `haskell_tree_sitter` module:

```python
# Current approach in haskell_support.py
ast_chunks = haskell_tree_sitter.get_haskell_ast_chunks_with_fallback(content)
```

**Future Enhancement**: Direct integration with tree-sitter Python bindings would allow:

- **Fine-grained AST control**: Direct node traversal and manipulation
- **Custom chunking strategies**: Based on specific AST node types
- **Better error handling**: Direct access to syntax error information  
- **Richer metadata**: AST node types, children, structural information
- **Performance**: Eliminate wrapper overhead

### Semantic Chunking vs Syntactic Chunking

**Current State**: Chunking is **syntactic** (based on code structure like functions, imports, data types)

**Future Enhancement**: **Semantic chunking** would group code by **meaning and relationships**:

#### Dependency-Based Chunking

```haskell
-- Instead of splitting these syntactically:
calculatePrice :: Product -> Price
validateOrder :: Order -> Bool 
processPayment :: Payment -> IO Result

-- Semantic chunking would group them as "order processing logic"
```

#### Type-Relationship Chunking

```haskell
-- Group data type with related functions:
data User = User { name :: String, email :: String }
validateUser :: User -> Bool
createUser :: String -> String -> User  
-- ^ These would be chunked together semantically
```

#### Domain Concept Clustering

- **Authentication**: login, logout, validateToken functions
- **Data Processing**: parse, transform, validate functions  
- **IO Operations**: read, write, network functions

The key insight is moving from **"where does the code split syntactically?"** to **"what code belongs together logically?"**

These enhancements would make chunking much more intelligent for downstream tasks like code search, documentation generation, and AI-assisted development.

## Conclusion

The enhanced Haskell chunking system successfully applies ASTChunk-inspired techniques to provide:

- **Superior chunk quality** through intelligent boundary detection
- **Rich metadata** enabling advanced downstream processing
- **Flexible configuration** for different use cases and requirements
- **Robust fallback strategies** ensuring reliability
- **Performance optimizations** for production usage
- **Haskell-specific intelligence** leveraging language characteristics

This enhancement positions CocoIndex's Haskell support at the same sophistication level as leading code analysis tools while maintaining the framework's flexibility and extensibility principles.

---

*This enhancement was implemented by analyzing and adapting techniques from the ASTChunk library, specifically focusing on configurable chunking, rich metadata generation, intelligent boundary detection, and multi-tier fallback strategies.*
